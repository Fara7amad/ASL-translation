{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'asl_alphabet_train/asl_alphabet_train'\n",
    "test_dir  = 'asl_alphabet_test/asl_alphabet_test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MediaPipe Hands solution in Python commonly used for detecting and tracking hands in images or videos.\n",
    "\n",
    "- mp_hands: imports the MediaPipe Hands module, for hand detection and tracking.\n",
    "- mp_drawing : to draw landmarks and connections on images, useful for visualizing the hand landmarks detected by the Hands module.\n",
    "- static_image_mode=True: \n",
    "  -  True : static image. \n",
    "  -  False : video stream and would use tracking to improve detection performance across frames.\n",
    "- max_num_hands=1: This limits the number of hands detected to a maximum of 1.\n",
    "- min_detection_confidence=0.7: This sets the minimum confidence threshold for the detection to be considered successful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands=mp.solutions.hands\n",
    "mp_drawing=mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process_image: function takes an image file path as input, processes the image to detect hand landmarks using MediaPipe, and returns the processed image along with the detected landmarks.\n",
    "\n",
    " - image = cv2.imread(image_path): This line uses OpenCV to read the image from the specified file path. The image is read in BGR format by default.\n",
    " - image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB): This converts the image from BGR to RGB format because the MediaPipe Hands module expects the input image in RGB format.\n",
    " - result = hands.process(image_rgb): This line processes the RGB image using the MediaPipe Hands module to detect hand landmarks. The result contains information about the detected landmarks if any hands are detected.\n",
    "\n",
    "If hand landmarks are detected, they are drawn on the original image, and their coordinates are collected in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_path):\n",
    "    image= cv2.imread(image_path)\n",
    "    image_rgb= cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "    result=hands.process(image_rgb)\n",
    "    \n",
    "    landmarks=[]\n",
    "    \n",
    "    if result.multi_hand_landmarks: # checks if any hand landmarks were detected in the image.\n",
    "        for hand_landmarks in result.multi_hand_landmarks: #loops through each detected hand\n",
    "            mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS) #to draw the detected hand landmarks and their connections on the original image.\n",
    "            for lm in hand_landmarks.landmark: #loops through each landmark\n",
    "                landmarks.append([lm.x, lm.y, lm.z])\n",
    "    return image, landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract_landmarks : function takes an image file path as input, processes the image to detect hand landmarks, and returns a flattened array of the landmark coordinates. If no landmarks are detected, it returns an array of zeros. \n",
    "\n",
    "- The processed image is ignored (denoted by _), and only the landmarks list is captured.\n",
    "\n",
    "- If landmarks were detected, converts the list of landmarks into a NumPy array and flattens it into a one-dimensional array. \n",
    "\n",
    "- Each landmark has 3 coordinates (x, y, z), and there are 21 landmarks, resulting in a flattened array of 63 elements (21 * 3).\n",
    "\n",
    "- If no landmarks are detected, it returns an array of zeros with a length of 63 (corresponding to 21 landmarks with x, y, and z coordinates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks(image_path):\n",
    "    _, landmarks = process_image(image_path)\n",
    "    if landmarks:\n",
    "        return np.array(landmarks).flatten()\n",
    "    else:\n",
    "        return np.zeros(63)  # 21 landmarks with x, y, z coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create datasets from directories of images, where each subdirectory corresponds to a class of images.\n",
    "\n",
    "processe each image to extract hand landmarks using the extract_landmarks function, \n",
    "\n",
    "label each set of landmarks according to its class, and saves the data to CSV files for later use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rahhal AbuZahra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(directory):\n",
    "    data = []\n",
    "    labels = []\n",
    "    classes = sorted(os.listdir(directory))\n",
    "    \n",
    "    for i, label in enumerate(classes):\n",
    "        class_dir = os.path.join(directory, label)\n",
    "        if os.path.isdir(class_dir):\n",
    "            for image_file in os.listdir(class_dir):\n",
    "                image_path = os.path.join(class_dir, image_file)\n",
    "                landmarks = extract_landmarks(image_path)\n",
    "                data.append(landmarks)\n",
    "                labels.append(i)\n",
    "    \n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "train_dir = 'asl_alphabet_train/asl_alphabet_train'\n",
    "test_dir = 'asl_alphabet_test/asl_alphabet_test'\n",
    "\n",
    "train_data, train_labels = create_dataset(train_dir)\n",
    "test_data, test_labels = create_dataset(test_dir)\n",
    "\n",
    "# Save to CSV for later use\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_df['label'] = train_labels\n",
    "train_df.to_csv('train_landmarks.csv', index=False)\n",
    "\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_df['label'] = test_labels\n",
    "test_df.to_csv('test_landmarks.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_df = pd.read_csv('train_landmarks.csv')\n",
    "test_df = pd.read_csv('test_landmarks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "\n",
    "#Extracts all columns except the last one (which is the label column) from the training DataFrame and converts them to a NumPy array.\n",
    "train_data = train_df.iloc[:, :-1].values \n",
    "#Extracts the label column from the training DataFrame and converts it to a NumPy array.\n",
    "train_labels = train_df['label'].values \n",
    "test_data = test_df.iloc[:, :-1].values\n",
    "test_labels = test_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical (one-hot encoding)\n",
    "num_classes = len(np.unique(train_labels))\n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
    "test_labels = to_categorical(test_labels, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple fully connected neural network\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rahhal AbuZahra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,885</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,192\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m)             │         \u001b[38;5;34m1,885\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,333</span> (71.61 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18,333\u001b[0m (71.61 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,333</span> (71.61 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m18,333\u001b[0m (71.61 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.2114 - loss: 2.6117 - val_accuracy: 0.0021 - val_loss: 13.1113\n",
      "Epoch 2/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.5441 - loss: 1.4320 - val_accuracy: 0.0653 - val_loss: 19.2627\n",
      "Epoch 3/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6067 - loss: 1.2620 - val_accuracy: 0.0651 - val_loss: 24.4291\n",
      "Epoch 4/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6356 - loss: 1.1913 - val_accuracy: 0.0852 - val_loss: 30.9028\n",
      "Epoch 5/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6545 - loss: 1.1356 - val_accuracy: 0.0862 - val_loss: 35.0922\n",
      "Epoch 6/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6612 - loss: 1.1089 - val_accuracy: 0.0786 - val_loss: 40.2940\n",
      "Epoch 7/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6720 - loss: 1.0800 - val_accuracy: 0.0845 - val_loss: 45.5468\n",
      "Epoch 8/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6736 - loss: 1.0796 - val_accuracy: 0.0765 - val_loss: 50.3388\n",
      "Epoch 9/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6749 - loss: 1.0772 - val_accuracy: 0.0741 - val_loss: 50.0554\n",
      "Epoch 10/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.6805 - loss: 1.0501 - val_accuracy: 0.0706 - val_loss: 53.4337\n",
      "Epoch 11/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.6856 - loss: 1.0489 - val_accuracy: 0.0841 - val_loss: 57.3936\n",
      "Epoch 12/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.6836 - loss: 1.0583 - val_accuracy: 0.0851 - val_loss: 57.7131\n",
      "Epoch 13/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.6898 - loss: 1.0333 - val_accuracy: 0.0851 - val_loss: 57.4665\n",
      "Epoch 14/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.6894 - loss: 1.0333 - val_accuracy: 0.0847 - val_loss: 61.2555\n",
      "Epoch 15/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.6887 - loss: 1.0390 - val_accuracy: 0.0850 - val_loss: 62.7002\n",
      "Epoch 16/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.6892 - loss: 1.0321 - val_accuracy: 0.0863 - val_loss: 59.8127\n",
      "Epoch 17/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.6926 - loss: 1.0235 - val_accuracy: 0.0856 - val_loss: 60.8152\n",
      "Epoch 18/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.6945 - loss: 1.0176 - val_accuracy: 0.0791 - val_loss: 62.8307\n",
      "Epoch 19/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.6928 - loss: 1.0238 - val_accuracy: 0.0770 - val_loss: 63.4365\n",
      "Epoch 20/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.6990 - loss: 1.0026 - val_accuracy: 0.0862 - val_loss: 65.1969\n",
      "Epoch 21/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.6932 - loss: 1.0247 - val_accuracy: 0.0861 - val_loss: 65.1420\n",
      "Epoch 22/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.6920 - loss: 1.0260 - val_accuracy: 0.0866 - val_loss: 67.2632\n",
      "Epoch 23/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.6950 - loss: 1.0215 - val_accuracy: 0.0861 - val_loss: 67.2822\n",
      "Epoch 24/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.6959 - loss: 1.0236 - val_accuracy: 0.0817 - val_loss: 71.7410\n",
      "Epoch 25/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7030 - loss: 0.9992 - val_accuracy: 0.0843 - val_loss: 68.5101\n",
      "Epoch 26/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.6980 - loss: 1.0105 - val_accuracy: 0.0702 - val_loss: 63.1530\n",
      "Epoch 27/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.6988 - loss: 1.0110 - val_accuracy: 0.0819 - val_loss: 65.2515\n",
      "Epoch 28/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.6990 - loss: 1.0055 - val_accuracy: 0.0863 - val_loss: 62.5875\n",
      "Epoch 29/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7004 - loss: 1.0050 - val_accuracy: 0.0795 - val_loss: 61.6504\n",
      "Epoch 30/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7022 - loss: 0.9939 - val_accuracy: 0.0848 - val_loss: 69.7559\n",
      "Epoch 31/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7013 - loss: 0.9996 - val_accuracy: 0.0793 - val_loss: 65.6477\n",
      "Epoch 32/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7032 - loss: 1.0001 - val_accuracy: 0.0852 - val_loss: 63.7720\n",
      "Epoch 33/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7020 - loss: 1.0004 - val_accuracy: 0.0852 - val_loss: 65.7453\n",
      "Epoch 34/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7029 - loss: 0.9977 - val_accuracy: 0.0794 - val_loss: 68.6212\n",
      "Epoch 35/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7012 - loss: 1.0049 - val_accuracy: 0.0853 - val_loss: 67.1005\n",
      "Epoch 36/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7042 - loss: 0.9903 - val_accuracy: 0.0811 - val_loss: 66.0011\n",
      "Epoch 37/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.6990 - loss: 1.0066 - val_accuracy: 0.0840 - val_loss: 71.2411\n",
      "Epoch 38/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7031 - loss: 0.9968 - val_accuracy: 0.0818 - val_loss: 63.9423\n",
      "Epoch 39/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7031 - loss: 0.9976 - val_accuracy: 0.0864 - val_loss: 70.2787\n",
      "Epoch 40/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7011 - loss: 1.0023 - val_accuracy: 0.0855 - val_loss: 68.9540\n",
      "Epoch 41/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7059 - loss: 0.9873 - val_accuracy: 0.0813 - val_loss: 68.9972\n",
      "Epoch 42/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7068 - loss: 0.9875 - val_accuracy: 0.0790 - val_loss: 71.7283\n",
      "Epoch 43/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7023 - loss: 0.9986 - val_accuracy: 0.0849 - val_loss: 70.4344\n",
      "Epoch 44/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7066 - loss: 0.9928 - val_accuracy: 0.0866 - val_loss: 75.6824\n",
      "Epoch 45/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7087 - loss: 0.9821 - val_accuracy: 0.0823 - val_loss: 71.7760\n",
      "Epoch 46/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7088 - loss: 0.9775 - val_accuracy: 0.0836 - val_loss: 70.3452\n",
      "Epoch 47/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7067 - loss: 0.9908 - val_accuracy: 0.0797 - val_loss: 71.6900\n",
      "Epoch 48/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7065 - loss: 0.9873 - val_accuracy: 0.0864 - val_loss: 71.5729\n",
      "Epoch 49/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7075 - loss: 0.9833 - val_accuracy: 0.0838 - val_loss: 67.6056\n",
      "Epoch 50/50\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7054 - loss: 0.9907 - val_accuracy: 0.0807 - val_loss: 71.7595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3929 - loss: 12.3935\n",
      "Test accuracy: 0.3928571343421936\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_data, train_labels, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Save the model\n",
    "model.save('asl_landmarks_model.h5')\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "print(f'Test accuracy: {test_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('final_model.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
